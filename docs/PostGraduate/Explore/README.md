---
title:  探索与记录
search: true

date: 2023-04-09 10:30:23
tags: []
photos:
description:
comments:
---

## 关键词整理
-  注意力模型《Attention Modal》（AM）
-  自注意力网络《self-Attention》
-  前馈神经网络《Feed Forward Neural Network》（FFNN/FFN）
   -  多层感知器《Multilayer Perceptron》（MLP）
   -  反向传播神经网络《Back Propagation Neural Network》（BP）
   -  径向基函数网络《Radial Basis Function Network》（RBF）
-  卷积神经网络《Convolutional Neural Networks》（CNN）
-  循环神经网络《Recurrent Neural Networks》（RNN）
-  递归神经网络《Recursive Neural Networks》（RNN）
-  

## 问题整理

1. [transformer为什么有利于并行计算？](https://www.zhihu.com/question/593941226)
- 当模型大了后，真正消耗算力（FLOPS）的，是多层感知器MLP（FFN），而自注意力（attn）和多头（MHA）只占小部分。所以，这个问题等价于`MLP有利于并行计算`。

1. [有哪些典型的神经网络](https://www.zhihu.com/question/519469199/answer/2466967964)

2. [2022年度AI论文Top10](https://zhuanlan.zhihu.com/p/542551226)
