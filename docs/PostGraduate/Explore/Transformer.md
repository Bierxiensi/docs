---
title:  Transformer模型学习记录
search: true

date: 2023-04-09 10:30:23
tags: []
photos:
description:
comments:
---


## Transformer
#### 引入
2018年谷歌团队提出了生成词向量算法BERT，其核心即为Transformer
#### 组成
自注意力机制`Self-Attention`和前馈神经网络`Feed Forward Neural Network`
#### 工作机制
对数据进行编码，捕获给定单词与其前后单词之间的关系
#### 优点
结合了CNN和RNN的优点
- 可以理解很远的序列元素之间的关系（克服了RNN缺点）
- 对序列中所有元素给予同样关注（自注意力机制？）
- 处理速度快（大模型算力消耗在并行计算，而MLP有利于并行计算？）
- 几乎可以处理任何序列数据（克服了CNN缺点）
- 可以实现上下文预测
- 有利于异常检测



## 注意力模型《Attention Model》（AM）
#### 引入
最早出现在机器翻译中，目前广泛使用在自然语言处理、统计学习、语音、计算机视觉
#### 功能：有选择的将注意力集中在某些部分，忽略其他不相关信息，有助于感知
#### 优势：
- 自然语言处理的新技术
- 提高了神经网络的可解释性
- 有助于克服RNN存在的一些问题