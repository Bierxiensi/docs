(window.webpackJsonp=window.webpackJsonp||[]).push([[117],{512:function(t,e,r){"use strict";r.r(e);var a=r(28),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h2",{attrs:{id:"关键词整理"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#关键词整理"}},[t._v("#")]),t._v(" 关键词整理")]),t._v(" "),r("ul",[r("li",[t._v("注意力模型《Attention Modal》（AM）")]),t._v(" "),r("li",[t._v("自注意力网络《self-Attention》")]),t._v(" "),r("li",[t._v("前馈神经网络《Feed Forward Neural Network》（FFNN/FFN）\n"),r("ul",[r("li",[t._v("多层感知器《Multilayer Perceptron》（MLP）")]),t._v(" "),r("li",[t._v("反向传播神经网络《Back Propagation Neural Network》（BP）")]),t._v(" "),r("li",[t._v("径向基函数网络《Radial Basis Function Network》（RBF）")])])]),t._v(" "),r("li",[t._v("卷积神经网络《Convolutional Neural Networks》（CNN）")]),t._v(" "),r("li",[t._v("循环神经网络《Recurrent Neural Networks》（RNN）")]),t._v(" "),r("li",[t._v("递归神经网络《Recursive Neural Networks》（RNN）")]),t._v(" "),r("li")]),t._v(" "),r("h2",{attrs:{id:"问题整理"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#问题整理"}},[t._v("#")]),t._v(" 问题整理")]),t._v(" "),r("ol",[r("li",[r("a",{attrs:{href:"https://www.zhihu.com/question/593941226",target:"_blank",rel:"noopener noreferrer"}},[t._v("transformer为什么有利于并行计算？"),r("OutboundLink")],1)])]),t._v(" "),r("ul",[r("li",[t._v("当模型大了后，真正消耗算力（FLOPS）的，是多层感知器MLP（FFN），而自注意力（attn）和多头（MHA）只占小部分。所以，这个问题等价于"),r("code",[t._v("MLP有利于并行计算")]),t._v("。")])]),t._v(" "),r("ol",[r("li",[r("p",[r("a",{attrs:{href:"https://www.zhihu.com/question/519469199/answer/2466967964",target:"_blank",rel:"noopener noreferrer"}},[t._v("有哪些典型的神经网络"),r("OutboundLink")],1)])]),t._v(" "),r("li",[r("p",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/542551226",target:"_blank",rel:"noopener noreferrer"}},[t._v("2022年度AI论文Top10"),r("OutboundLink")],1)])])])])}),[],!1,null,null,null);e.default=n.exports}}]);